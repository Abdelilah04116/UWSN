# ğŸŒŠ Optimisation de Routage UWSN avec PPO (Deep Reinforcement Learning)

Ce projet implÃ©mente une solution complÃ¨te d'optimisation de routage dans les rÃ©seaux de capteurs sous-marins (UWSN) en utilisant l'algorithme PPO (Proximal Policy Optimization) de Deep Reinforcement Learning.

## ğŸ“‹ Table des matiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [FonctionnalitÃ©s](#fonctionnalitÃ©s)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Structure du projet](#structure-du-projet)
- [ModÃ¨les physiques](#modÃ¨les-physiques)
- [Exemples](#exemples)
- [Contributions](#contributions)

## ğŸ¯ Vue d'ensemble

Les rÃ©seaux de capteurs sous-marins (UWSN) prÃ©sentent des dÃ©fis uniques pour l'optimisation de routage :

- **Communication acoustique** : Propagation complexe avec pertes et absorption
- **Consommation Ã©nergÃ©tique** : Batteries limitÃ©es, recharge difficile
- **Environnement dynamique** : TempÃ©rature, salinitÃ©, profondeur variables
- **Contraintes physiques** : Latence Ã©levÃ©e, bande passante limitÃ©e

Ce projet utilise l'apprentissage par renforcement pour trouver des chemins optimaux qui minimisent la consommation Ã©nergÃ©tique tout en respectant les contraintes acoustiques.

## âœ¨ FonctionnalitÃ©s

### ğŸ”¬ ModÃ¨les Physiques RÃ©alistes
- **Propagation acoustique** : Ã‰quations de Mackenzie et Francois & Garrison
- **Consommation Ã©nergÃ©tique** : ModÃ¨les basÃ©s sur la distance et la puissance
- **Environnement dynamique** : TempÃ©rature, salinitÃ©, profondeur variables

### ğŸ¤– Deep Reinforcement Learning
- **Algorithme PPO** : Stable-Baselines3 avec PyTorch
- **Environnement Gym** : Interface standardisÃ©e pour l'entraÃ®nement
- **Fonction de rÃ©compense** : Optimisation multi-objectifs

### ğŸ“Š Visualisations Interactives
- **RÃ©seau 3D** : Visualisation Plotly interactive
- **MÃ©triques en temps rÃ©el** : Consommation, latence, succÃ¨s
- **Comparaisons** : PPO vs mÃ©thodes de baseline

### ğŸŒ Interface Utilisateur
- **Streamlit** : Application web interactive
- **Google Colab** : Notebook prÃªt Ã  l'emploi
- **Configuration flexible** : ParamÃ¨tres rÃ©seau et physiques

## ğŸš€ Installation

### PrÃ©requis
- Python 3.8+
- PyTorch 2.0+
- CUDA (optionnel, pour l'accÃ©lÃ©ration GPU)

### Installation des dÃ©pendances

```bash
# Cloner le repository
git clone https://github.com/votre-username/uwsn-ppo-routing.git
cd uwsn-ppo-routing

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### DÃ©pendances principales
```
gym==0.21.0
stable-baselines3==2.0.0
torch==2.0.1
numpy==1.24.3
matplotlib==3.7.1
plotly==5.15.0
streamlit==1.25.0
pandas==2.0.3
scipy==1.11.1
```

## ğŸ“– Utilisation

### 1. Notebook Google Colab

Le moyen le plus rapide de commencer :

1. Ouvrir `notebooks/uwsn_ppo_colab.ipynb` dans Google Colab
2. ExÃ©cuter toutes les cellules
3. Suivre les instructions pour l'entraÃ®nement et l'Ã©valuation

### 2. Application Streamlit

Interface web interactive :

```bash
# Lancer l'application Streamlit
streamlit run app/streamlit_app.py
```

FonctionnalitÃ©s de l'interface :
- Configuration du rÃ©seau (nombre de nÅ“uds, zone, profondeur)
- SÃ©lection source/destination
- Chargement de modÃ¨les PPO
- Visualisation 3D/2D interactive
- MÃ©triques dÃ©taillÃ©es

### 3. EntraÃ®nement personnalisÃ©

```python
from src.ppo_train import UWSNTrainer

# CrÃ©er un trainer
trainer = UWSNTrainer(
    num_nodes=15,
    area_size=1000.0,
    depth_range=(-100, -10)
)

# EntraÃ®ner le modÃ¨le
model = trainer.train(total_timesteps=200000)

# Ã‰valuer
metrics = trainer.evaluate(num_episodes=100)
```

### 4. Utilisation de l'environnement

```python
from src.env_gym import UWSNRoutingEnv
from src.utils_network import create_sample_network

# CrÃ©er un rÃ©seau
nodes = create_sample_network(num_nodes=10)

# CrÃ©er l'environnement
env = UWSNRoutingEnv(nodes=nodes)

# Simulation
obs = env.reset()
done = False
while not done:
    action = env.action_space.sample()  # Action alÃ©atoire
    obs, reward, done, info = env.step(action)
```

## ğŸ“ Structure du projet

```
uwsn-ppo-routing/
â”œâ”€â”€ src/                          # Code source principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ env_gym.py               # Environnement Gym personnalisÃ©
â”‚   â”œâ”€â”€ ppo_train.py             # Script d'entraÃ®nement PPO
â”‚   â””â”€â”€ utils_network.py         # Fonctions utilitaires
â”œâ”€â”€ app/                         # Application Streamlit
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ streamlit_app.py         # Interface web
â”œâ”€â”€ notebooks/                   # Notebooks Jupyter
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ uwsn_ppo_colab.ipynb    # Notebook Google Colab
â”œâ”€â”€ models/                      # ModÃ¨les entraÃ®nÃ©s
â”‚   â””â”€â”€ ppo_uwsn.zip            # ModÃ¨le PPO sauvegardÃ©
â”œâ”€â”€ requirements.txt             # DÃ©pendances Python
â””â”€â”€ README.md                   # Documentation
```

## ğŸ”¬ ModÃ¨les physiques

### Propagation Acoustique

#### Vitesse du son (Ã‰quation de Mackenzie)
```
c(T,S,D) = 1448.96 + 4.591*T - 5.304Ã—10â»Â²*TÂ² + 2.374Ã—10â»â´*TÂ³
           + 1.340*(S-35) + 1.630Ã—10â»Â²*D + 1.675Ã—10â»â·*DÂ²
           - 1.025Ã—10â»Â²*T*(S-35) - 7.139Ã—10â»Â¹Â³*T*DÂ³
```

OÃ¹ :
- T : TempÃ©rature (Â°C)
- S : SalinitÃ© (PSU)
- D : Profondeur (m)

#### Absorption acoustique (Francois & Garrison)
```
Î±(f,T,S,D) = Aâ‚*Pâ‚*fâ‚*fÂ²/(fâ‚Â² + fÂ²) + Aâ‚‚*Pâ‚‚*fâ‚‚*fÂ²/(fâ‚‚Â² + fÂ²) + Aâ‚ƒ*Pâ‚ƒ*fÂ²
```

#### Perte de trajet
```
TL = 20*logâ‚â‚€(d) + Î±*d/1000
```

### Consommation Ã‰nergÃ©tique

#### Transmission
```
E_tx = (E_elec + E_amp * dÂ²) * k
```

#### RÃ©ception
```
E_rx = E_elec * k
```

OÃ¹ :
- E_elec = 50 nJ/bit (Ã©lectronique)
- E_amp = 1 pJ/bit/mÂ² (amplification)
- d : Distance (m)
- k : Taille des donnÃ©es (bits)

## ğŸ® Fonction de rÃ©compense PPO

La rÃ©compense est calculÃ©e comme suit :

```python
reward = (
    -total_energy / 1000.0 +           # Minimiser l'Ã©nergie
    -distance / 1000.0 +               # PÃ©nalitÃ© distance
    100.0 if success else 0.0 +        # RÃ©compense succÃ¨s
    -5.0 if visited_penalty else 0.0 + # PÃ©nalitÃ© boucles
    -10.0 if low_energy else 0.0 +     # PÃ©nalitÃ© Ã©nergie faible
    proximity_improvement / 100.0      # RÃ©compense proximitÃ©
)
```

## ğŸ“Š Exemples

### Exemple 1 : RÃ©seau simple

```python
# CrÃ©er un rÃ©seau de 10 nÅ“uds
nodes = create_sample_network(num_nodes=10, area_size=500.0)

# CrÃ©er l'environnement
env = UWSNRoutingEnv(nodes=nodes)

# Simulation d'un Ã©pisode
obs = env.reset()
done = False
total_reward = 0

while not done:
    action = env.action_space.sample()
    obs, reward, done, info = env.step(action)
    total_reward += reward

print(f"RÃ©compense totale: {total_reward:.2f}")
print(f"Chemin: {info['episode_stats']['path']}")
```

### Exemple 2 : EntraÃ®nement rapide

```python
# Configuration minimale pour test
trainer = UWSNTrainer(num_nodes=8, area_size=500.0)

# EntraÃ®nement rapide (5 minutes)
model = trainer.train(total_timesteps=50000)

# Ã‰valuation
metrics = trainer.evaluate(num_episodes=50)
print(f"Taux de succÃ¨s: {metrics['success_rate']:.2%}")
```

### Exemple 3 : Visualisation

```python
import plotly.graph_objects as go

# CrÃ©er la visualisation 3D
fig = go.Figure()

# Ajouter les nÅ“uds
for node in nodes:
    fig.add_trace(go.Scatter3d(
        x=[node.x], y=[node.y], z=[node.z],
        mode='markers',
        marker=dict(size=10, color=node.energy),
        text=f"NÅ“ud {node.id}<br>Ã‰nergie: {node.energy:.1f}J"
    ))

fig.show()
```

## ğŸ¯ MÃ©triques de performance

### MÃ©triques principales
- **Taux de succÃ¨s** : Pourcentage d'Ã©pisodes rÃ©ussis
- **Consommation Ã©nergÃ©tique** : Ã‰nergie totale (J)
- **Latence** : Temps de transmission (s)
- **EfficacitÃ©** : Ã‰nergie par bit transmis

### Comparaisons avec baselines
- **Chemin le plus court** : Dijkstra
- **Politique alÃ©atoire** : Actions alÃ©atoires
- **PPO** : Notre mÃ©thode

## ğŸ”§ Configuration avancÃ©e

### ParamÃ¨tres d'entraÃ®nement PPO

```python
model = PPO(
    "MlpPolicy",
    env,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.01,
    vf_coef=0.5
)
```

### ParamÃ¨tres du rÃ©seau

```python
# Configuration rÃ©seau
num_nodes = 15          # Nombre de nÅ“uds
area_size = 1000.0      # Taille de zone (m)
depth_range = (-100, -10)  # Profondeur (m)
data_size = 1000        # Taille donnÃ©es (bits)
frequency = 25.0        # FrÃ©quence acoustique (kHz)
```

## ğŸ› DÃ©pannage

### ProblÃ¨mes courants

1. **Erreur CUDA** : Installer PyTorch CPU-only
2. **MÃ©moire insuffisante** : RÃ©duire `num_nodes` ou `batch_size`
3. **EntraÃ®nement lent** : Utiliser GPU ou rÃ©duire `total_timesteps`

### Logs et dÃ©bogage

```python
# Activer les logs dÃ©taillÃ©s
import logging
logging.basicConfig(level=logging.DEBUG)

# VÃ©rifier l'environnement
env = UWSNRoutingEnv(nodes=nodes)
print(f"Espace d'observation: {env.observation_space}")
print(f"Espace d'action: {env.action_space}")
```

## ğŸ¤ Contributions

Les contributions sont les bienvenues ! Voici comment contribuer :

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Commit les changements (`git commit -am 'Ajouter nouvelle fonctionnalitÃ©'`)
4. Push vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. CrÃ©er une Pull Request

### IdÃ©es d'amÃ©liorations
- [ ] Support de rÃ©seaux dynamiques
- [ ] IntÃ©gration de contraintes de QoS
- [ ] Optimisation multi-objectifs
- [ ] Support de protocoles de routage existants
- [ ] Tests unitaires complets

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ“š RÃ©fÃ©rences

1. **PPO** : Schulman, J., et al. "Proximal policy optimization algorithms." arXiv preprint arXiv:1707.06347 (2017).

2. **Propagation acoustique** : Mackenzie, K. V. "Nine-term equation for sound speed in the oceans." The Journal of the Acoustical Society of America 70.3 (1981): 807-812.

3. **Absorption acoustique** : Francois, R. E., and G. R. Garrison. "Sound absorption based on ocean measurements. Part II: Boric acid contribution and equation for total absorption." The Journal of the Acoustical Society of America 72.6 (1982): 1879-1890.

4. **UWSN** : Akyildiz, I. F., et al. "Underwater acoustic sensor networks: research challenges." Ad hoc networks 3.3 (2005): 257-279.

## ğŸ“ Contact

- **Auteur** : Abdelilah ourti
- **Email** : abdelilahourti@gmail.com

---

â­ N'hÃ©sitez pas Ã  donner une Ã©toile si ce projet vous a aidÃ© !
