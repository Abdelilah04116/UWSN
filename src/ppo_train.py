### Fichier: src/ppo_train.py

"""
Script d'entra√Ænement PPO pour l'optimisation de routage UWSN
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Tuple
import pickle
import json
from datetime import datetime

import torch
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv

from .env_gym import UWSNRoutingEnv
from .utils_network import create_sample_network, Node


class UWSNTrainer:
    """
    Classe pour l'entra√Ænement du mod√®le PPO sur l'environnement UWSN
    """
    
    def __init__(self, num_nodes: int = 10, area_size: float = 1000.0, 
                 depth_range: Tuple[float, float] = (-100, -10),
                 model_save_path: str = "models/ppo_uwsn"):
        """
        Initialise le trainer UWSN
        
        Args:
            num_nodes: Nombre de n≈ìuds dans le r√©seau
            area_size: Taille de la zone (m)
            depth_range: Plage de profondeur (m)
            model_save_path: Chemin de sauvegarde du mod√®le
        """
        self.num_nodes = num_nodes
        self.area_size = area_size
        self.depth_range = depth_range
        self.model_save_path = model_save_path
        
        # Cr√©ation du r√©seau
        self.nodes = create_sample_network(num_nodes, area_size, depth_range)
        
        # Environnement
        self.env = None
        self.model = None
        
        # Historique d'entra√Ænement
        self.training_history = {
            'episode_rewards': [],
            'episode_lengths': [],
            'success_rates': [],
            'energy_consumption': [],
            'evaluation_rewards': []
        }
        
        # Cr√©ation du dossier de sauvegarde
        os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    
    def create_environment(self, max_steps: int = 50) -> UWSNRoutingEnv:
        """Cr√©e l'environnement UWSN"""
        return UWSNRoutingEnv(
            nodes=self.nodes,
            max_steps=max_steps,
            data_size_range=(500, 2000)
        )
    
    def train(self, total_timesteps: int = 100000, 
              learning_rate: float = 3e-4,
              n_steps: int = 2048,
              batch_size: int = 64,
              n_epochs: int = 10,
              gamma: float = 0.99,
              gae_lambda: float = 0.95,
              clip_range: float = 0.2,
              ent_coef: float = 0.01,
              vf_coef: float = 0.5,
              max_grad_norm: float = 0.5,
              eval_freq: int = 10000,
              save_freq: int = 50000) -> PPO:
        """
        Entra√Æne le mod√®le PPO
        
        Args:
            total_timesteps: Nombre total de pas d'entra√Ænement
            learning_rate: Taux d'apprentissage
            n_steps: Nombre de pas par mise √† jour
            batch_size: Taille du batch
            n_epochs: Nombre d'√©poques par mise √† jour
            gamma: Facteur de remise
            gae_lambda: Param√®tre GAE
            clip_range: Plage de clipping PPO
            ent_coef: Coefficient d'entropie
            vf_coef: Coefficient de la fonction de valeur
            max_grad_norm: Norme maximale du gradient
            eval_freq: Fr√©quence d'√©valuation
            save_freq: Fr√©quence de sauvegarde
        
        Returns:
            Mod√®le PPO entra√Æn√©
        """
        print("üöÄ D√©but de l'entra√Ænement PPO pour UWSN...")
        print(f"üìä Configuration: {self.num_nodes} n≈ìuds, {total_timesteps} pas")
        
        # Cr√©ation de l'environnement
        self.env = self.create_environment()
        
        # Environnement d'√©valuation
        eval_env = Monitor(self.create_environment())
        
        # Configuration PPO
        policy_kwargs = dict(
            net_arch=[dict(pi=[256, 256], vf=[256, 256])],
            activation_fn=torch.nn.ReLU
        )
        
        # S√©lection du device (GPU si dispo)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Cr√©ation du mod√®le PPO
        self.model = PPO(
            "MlpPolicy",
            self.env,
            learning_rate=learning_rate,
            n_steps=n_steps,
            batch_size=batch_size,
            n_epochs=n_epochs,
            gamma=gamma,
            gae_lambda=gae_lambda,
            clip_range=clip_range,
            ent_coef=ent_coef,
            vf_coef=vf_coef,
            max_grad_norm=max_grad_norm,
            policy_kwargs=policy_kwargs,
            device=device,
            verbose=1,
            tensorboard_log="./tensorboard_logs/"
        )
        
        # Callback d'√©valuation
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=f"{self.model_save_path}_best",
            log_path=f"{self.model_save_path}_logs",
            eval_freq=eval_freq,
            deterministic=True,
            render=False
        )
        
        # Entra√Ænement
        print("üèãÔ∏è Entra√Ænement en cours...")
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=eval_callback,
            progress_bar=False
        )
        
        # Sauvegarde du mod√®le final
        self.model.save(self.model_save_path)
        print(f"üíæ Mod√®le sauvegard√©: {self.model_save_path}")

        # Export convivial du meilleur mod√®le √©valu√© si disponible
        try:
            import shutil, os
            best_ckpt = os.path.join(f"{self.model_save_path}_best", "best_model.zip")
            export_best = f"{self.model_save_path}_best.zip"
            if os.path.exists(best_ckpt):
                shutil.copyfile(best_ckpt, export_best)
                print(f"üèÖ Meilleur mod√®le export√© vers: {export_best}")
            else:
                print("‚ÑπÔ∏è Aucun best_model.zip d√©tect√© (√©valuation pas encore produite).")
        except Exception as e:
            print(f"‚ö†Ô∏è Export du meilleur mod√®le √©chou√©: {e}")
        
        # Sauvegarde des m√©tadonn√©es
        self._save_metadata()
        
        return self.model
    
    def evaluate(self, num_episodes: int = 100, render: bool = False) -> Dict[str, Any]:
        """
        √âvalue le mod√®le entra√Æn√©
        
        Args:
            num_episodes: Nombre d'√©pisodes d'√©valuation
            render: Afficher les √©pisodes
        
        Returns:
            Dictionnaire des m√©triques d'√©valuation
        """
        if self.model is None:
            raise ValueError("Le mod√®le doit √™tre entra√Æn√© avant l'√©valuation")
        
        print(f"üîç √âvaluation sur {num_episodes} √©pisodes...")
        
        eval_env = self.create_environment()
        
        episode_rewards = []
        episode_lengths = []
        success_rates = []
        energy_consumptions = []
        paths = []
        
        for episode in range(num_episodes):
            obs, _ = eval_env.reset()
            terminated, truncated = False, False
            episode_reward = 0.0
            step_count = 0
            
            while not (terminated or truncated):
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                episode_reward += reward
                step_count += 1
                
                if render:
                    eval_env.render()
            
            # Collecte des m√©triques
            episode_rewards.append(episode_reward)
            episode_lengths.append(step_count)
            
            if 'episode_stats' in info:
                stats = info['episode_stats']
                success_rates.append(1.0 if stats['success'] else 0.0)
                energy_consumptions.append(stats['total_energy'])
                paths.append(stats['path'])
        
        # Calcul des m√©triques moyennes
        metrics = {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'std_length': np.std(episode_lengths),
            'success_rate': np.mean(success_rates),
            'mean_energy': np.mean(energy_consumptions),
            'std_energy': np.std(energy_consumptions),
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'success_rates': success_rates,
            'energy_consumptions': energy_consumptions,
            'paths': paths
        }
        
        print(f"üìà R√©sultats d'√©valuation:")
        print(f"   R√©compense moyenne: {metrics['mean_reward']:.2f} ¬± {metrics['std_reward']:.2f}")
        print(f"   Longueur moyenne: {metrics['mean_length']:.2f} ¬± {metrics['std_length']:.2f}")
        print(f"   Taux de succ√®s: {metrics['success_rate']:.2%}")
        print(f"   √ânergie moyenne: {metrics['mean_energy']:.2f} ¬± {metrics['std_energy']:.2f}")
        
        return metrics
    
    def compare_with_baseline(self, num_episodes: int = 100) -> Dict[str, Any]:
        """
        Compare le mod√®le PPO avec des m√©thodes de baseline
        
        Args:
            num_episodes: Nombre d'√©pisodes pour la comparaison
        
        Returns:
            Dictionnaire des r√©sultats de comparaison
        """
        print("üÜö Comparaison avec les m√©thodes de baseline...")
        
        # √âvaluation du mod√®le PPO
        ppo_metrics = self.evaluate(num_episodes)
        
        # Baseline: Chemin le plus court (Dijkstra)
        eval_env = self.create_environment()
        baseline_metrics = self._evaluate_baseline(eval_env, num_episodes)
        
        # Baseline: Al√©atoire
        random_metrics = self._evaluate_random(eval_env, num_episodes)
        
        comparison = {
            'ppo': ppo_metrics,
            'shortest_path': baseline_metrics,
            'random': random_metrics
        }
        
        # Affichage des r√©sultats
        print("\nüìä Comparaison des m√©thodes:")
        print(f"{'M√©thode':<15} {'R√©compense':<12} {'Succ√®s':<8} {'√ânergie':<10}")
        print("-" * 50)
        print(f"{'PPO':<15} {ppo_metrics['mean_reward']:<12.2f} {ppo_metrics['success_rate']:<8.2%} {ppo_metrics['mean_energy']:<10.2f}")
        print(f"{'Plus court':<15} {baseline_metrics['mean_reward']:<12.2f} {baseline_metrics['success_rate']:<8.2%} {baseline_metrics['mean_energy']:<10.2f}")
        print(f"{'Al√©atoire':<15} {random_metrics['mean_reward']:<12.2f} {random_metrics['success_rate']:<8.2%} {random_metrics['mean_energy']:<10.2f}")
        
        return comparison
    
    def _evaluate_baseline(self, env: UWSNRoutingEnv, num_episodes: int) -> Dict[str, Any]:
        """√âvalue la m√©thode du chemin le plus court"""
        episode_rewards = []
        success_rates = []
        energy_consumptions = []
        
        for episode in range(num_episodes):
            obs, _ = env.reset()
            destination = env.destination
            
            # Chemin optimal
            optimal_path = env.get_optimal_path()
            
            if len(optimal_path) < 2:
                episode_rewards.append(-1000)
                success_rates.append(0.0)
                energy_consumptions.append(float('inf'))
                continue
            
            total_reward = 0.0
            info = {}
            for i in range(len(optimal_path) - 1):
                action = optimal_path[i + 1]
                obs, reward, terminated, truncated, info = env.step(action)
                total_reward += reward
                if terminated or truncated:
                    break
            
            episode_rewards.append(total_reward)
            success_rates.append(1.0 if env.state.current_node == destination else 0.0)
            energy_consumptions.append(info.get('episode_stats', {}).get('total_energy', 0.0))
        
        return {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'success_rate': np.mean(success_rates),
            'mean_energy': np.mean(energy_consumptions),
            'std_energy': np.std(energy_consumptions)
        }
    
    def _evaluate_random(self, env: UWSNRoutingEnv, num_episodes: int) -> Dict[str, Any]:
        """√âvalue une politique al√©atoire"""
        episode_rewards = []
        success_rates = []
        energy_consumptions = []
        
        for episode in range(num_episodes):
            obs, _ = env.reset()
            terminated, truncated = False, False
            total_reward = 0.0
            step_count = 0
            max_steps = 50
            
            while not (terminated or truncated) and step_count < max_steps:
                valid_actions = [i for i in range(env.num_nodes) if env.nodes[i].is_alive()]
                if not valid_actions:
                    break
                action = np.random.choice(valid_actions)
                obs, reward, terminated, truncated, info = env.step(action)
                total_reward += reward
                step_count += 1
            
            episode_rewards.append(total_reward)
            success_rates.append(1.0 if env.state.current_node == env.destination else 0.0)
            energy_consumptions.append(info.get('episode_stats', {}).get('total_energy', 0.0))
        
        return {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'success_rate': np.mean(success_rates),
            'mean_energy': np.mean(energy_consumptions),
            'std_energy': np.std(energy_consumptions)
        }
    
    def plot_training_curves(self, save_path: str = None):
        """Affiche les courbes d'entra√Ænement"""
        if not self.training_history['episode_rewards']:
            print("Aucune donn√©e d'entra√Ænement disponible")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # R√©compenses par √©pisode
        axes[0, 0].plot(self.training_history['episode_rewards'])
        axes[0, 0].set_title('R√©compenses par √©pisode')
        axes[0, 0].set_xlabel('√âpisode')
        axes[0, 0].set_ylabel('R√©compense')
        axes[0, 0].grid(True)
        
        # Longueur des √©pisodes
        axes[0, 1].plot(self.training_history['episode_lengths'])
        axes[0, 1].set_title('Longueur des √©pisodes')
        axes[0, 1].set_xlabel('√âpisode')
        axes[0, 1].set_ylabel('Longueur')
        axes[0, 1].grid(True)
        
        # Taux de succ√®s
        axes[1, 0].plot(self.training_history['success_rates'])
        axes[1, 0].set_title('Taux de succ√®s')
        axes[1, 0].set_xlabel('√âpisode')
        axes[1, 0].set_ylabel('Taux de succ√®s')
        axes[1, 0].grid(True)
        
        # Consommation √©nerg√©tique
        axes[1, 1].plot(self.training_history['energy_consumption'])
        axes[1, 1].set_title('Consommation √©nerg√©tique')
        axes[1, 1].set_xlabel('√âpisode')
        axes[1, 1].set_ylabel('√ânergie (J)')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"üìä Graphiques sauvegard√©s: {save_path}")
        
        plt.show()
    
    def _save_metadata(self):
        """Sauvegarde les m√©tadonn√©es du mod√®le"""
        metadata = {
            'num_nodes': self.num_nodes,
            'area_size': self.area_size,
            'depth_range': self.depth_range,
            'training_date': datetime.now().isoformat(),
            'model_path': self.model_save_path,
            'nodes_info': [
                {
                    'id': node.id,
                    'position': [node.x, node.y, node.z],
                    'energy': node.energy,
                    'temperature': node.temperature,
                    'salinity': node.salinity
                }
                for node in self.nodes
            ]
        }
        
        metadata_path = f"{self.model_save_path}_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"üìã M√©tadonn√©es sauvegard√©es: {metadata_path}")


def main():
    """Fonction principale pour l'entra√Ænement"""
    print("üåä Entra√Ænement PPO pour l'optimisation de routage UWSN")
    print("=" * 60)
    
    # Configuration
    num_nodes = 15
    area_size = 1000.0
    depth_range = (-100, -10)
    total_timesteps = 20000
    
    # Cr√©ation du trainer
    trainer = UWSNTrainer(
        num_nodes=num_nodes,
        area_size=area_size,
        depth_range=depth_range,
        model_save_path="models/ppo_uwsn"
    )
    
    # Entra√Ænement
    model = trainer.train(total_timesteps=total_timesteps)
    
    # √âvaluation
    print("\n" + "=" * 60)
    print("üîç √âvaluation du mod√®le entra√Æn√©")
    metrics = trainer.evaluate(num_episodes=50)
    
    # Comparaison avec les baselines
    print("\n" + "=" * 60)
    print("üÜö Comparaison avec les m√©thodes de baseline")
    comparison = trainer.compare_with_baseline(num_episodes=100)
    
    # Affichage des courbes d'entra√Ænement
    trainer.plot_training_curves("training_curves.png")
    
    print("\n‚úÖ Entra√Ænement termin√© avec succ√®s!")
    print(f"üíæ Mod√®le sauvegard√©: {trainer.model_save_path}")


if __name__ == "__main__":
    main()
